Resources
===========

epoll编程介绍
.............

epoll是Linux内核为处理大批量文件描述符而作了改进的poll，是Linux下多路复用IO接口select/poll的增强版本，
它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。另一点原因就是获取事件的时候，它无须遍历
整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。epoll除了提供
select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得
用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。

优点
-----

1. 支持一个进程打开大数目的socket描述符

2. 使用epoll进行高性能网络编程

    select 最不能忍受的是一个进程所打开的FD是有一定限制的，由FD_SETSIZE设置，默认值是1024。对于那些需要支持的
    上万连接数目的IM服务器来说显然太少了。这时候你一是可以选择修改这个宏然后重新编译服务器代码，不过资料也同时指出
    这样会带来网络效率的下降，二是可以选择多进程的解决方案（传统的Apache方案），不过虽然linux上面创建进程的代价
    比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。不过 epoll
    则没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子，在1GB内存的机器
    上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max查看，一般来说这个数目和系统内存关系很大。

3. IO效率不随FD数目增加而线性下降

    传统的select/poll另一个致命弱点就是当你拥有一个很大的socket集合，不过由于网络延时，任一时间只有部分的socket
    是“活跃”的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。但是epoll不存在这个问题，它
    只会对“活跃”的socket进行操作---这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。那么，只有
    “活跃”的socket才会主动的去调用 callback函数，其他idle状态socket则不会，在这点上，epoll实现了一个“伪”AIO
    ，因为这时候推动力在os内核。在一些 benchmark中，如果所有的socket基本上都是活跃的---比如一个高速LAN环境，
    epoll并不比select/poll有什么效率，相反，如果过多使用epoll_ctl,效率相比还有稍微的下降。但是一旦使用
    idle connections模拟WAN环境，epoll的效率就远在select/poll之上了。

4. 没有使用mmap加速内核与用户空间的消息传递

5. 内核微调

    这一点其实不算epoll的优点了，而是整个linux平台的优点。也许你可以怀疑linux平台，但是你无法回避linux平台赋予
    你微调内核的能力。比如，内核TCP/IP协议栈使用内存池管理sk_buff结构，那么可以在运行时期动态调整这个内存
    pool（skb_head_pool）的大小--- 通过echo XXXX>/proc/sys/net/core/hot_list_length完成。再比如
    listen函数的第2个参数（TCP完成3次握手的数据包队列长度），也可以根据你平台内存大小动态调整。更甚至在一个
    数据包个数目巨大但同时每个数据包本身大小却很小的特殊系统上尝试最新的NAPI网卡驱动架构。


由于Linux能调整参数，epoll不属于进程，线程，所以占用系统资源很少，所以这样就能提高服务器的在线数，由于是单线程，所以
这种epoll适合活跃数少连接数多的应用。

TCP编程注意事项
................

TCP的发送缓冲区和接收缓冲区
-------------------------

TCP协议是作用是用来进行端对端数据传送的，那么就会有发送端和接收端，在操作系统有两个空间即user space和kernal space。

每个Tcp socket连接在内核中都有一个发送缓冲区和接收缓冲区，TCP的全双工的工作模式以及TCP的流量(拥塞)控制便是依赖于这两个独立的buffer
以及buffer的填充状态。

单工：只允许甲方向乙方传送信息，而乙方不能向甲方传送 ，如汽车单行道。

半双工：半双工就是指一个时间段内只有一个动作发生，甲方可以向乙方传送数据，乙方也可以向甲方传送数据，但不能同时进行，如一条窄马路同一时间
只能允许一个车通行。

全双工：同时允许数据在两个方向上同时传输，它在能力上相当于两个单工通信方式的结合。

一个socket的两端，都会有send和recv两个方法，如client发送数据到server，那么就是客户端进程调用send发送数据，而send的作用是将数据拷贝
进入socket的内核发送缓冲区之中，然后send便会在上层返回。

也就是说send()方法返回之时，数据不一定会

发送到对端即服务器上去（和write写文件有点类似），send()仅仅是把应用层buffer的数据拷贝进socket的内核发送buffer中，发送是TCP的事情，
和send其实没有太大关系。

接收缓冲区把数据缓存入内核，等待recv()读取，recv()所做的工作，就是把内核缓冲区中的数据拷贝到应用层用户的buffer里面，并返回。若应用进
程一直没有调用recv()进行读取的话，此数据会一直缓存在相应socket的接收缓冲区内。对于TCP，如果应用进程一直没有读取，接收缓冲区满了之后，
发生的动作是：收端通知发端，接收窗口关闭（win=0）。这个便是滑动窗口的实现。保证TCP套接口接收缓冲区不会溢出，从而保证了TCP是可靠传输。
因为对方不允许发出超过所通告窗口大小的数据。 这就是TCP的流量控制，如果对方无视窗口大小而发出了超过窗口大小的数据，则接收方TCP将丢弃它。

查看socket发送缓冲区大小，cat /proc/sys/net/ipv4/tcp_wmem

python中的send函数遇到发送缓冲区满之后就会报错，BlockingIOError: [Errno 11] Resource temporarily unavailable，这是异步的
socket对象调用send方法导致的，解决的方法是，首先获取该socket对象的发送缓冲区的剩余大小，然后发送对应大小的数据，如果数据很大，就循环
发，直到发完为止。解决问题的方式是，在发送和接收数据之前设置缓冲区的大小，但是这样又会导致另外的问题。